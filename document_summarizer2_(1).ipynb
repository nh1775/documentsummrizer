{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nh1775/documentsummrizer/blob/main/document_summarizer2_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "dF2VyFNyfljz",
        "outputId": "f2460992-1286-488c-e429-f5c82eef4fd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-adaf062c-d8db-4e73-84db-5b1407eebf66\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-adaf062c-d8db-4e73-84db-5b1407eebf66\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "# Get the filename from the uploaded dictionary\n",
        "#pdf_path = next(iter(uploaded.keys()))\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Define the target directory\n",
        "target_dir = '/content/drive_2/MyDrive/DocumentSummarizer/'\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Define the full path for the new location\n",
        "target_path = f'{target_dir}{pdf_path}'\n",
        "print(\"pdf_path\",target_path)\n",
        "\n",
        "# Move the file to the target directory\n",
        "shutil.move(pdf_path, target_path)\n",
        "\n",
        "all_text_path = os.path.join(target_dir, 'all_text_path.txt')\n",
        "In_between_output = os.path.join(target_dir, 'In_between_output.txt')\n",
        "adib_summary = os.path.join(target_dir, 'adib_summary.txt')\n",
        "adib_summary2 = os.path.join(target_dir, 'adib_summary2.txt')\n",
        "adib_titles = os.path.join(target_dir, 'adib_titles.txt')\n",
        "content = \"\"\n",
        "with open(all_text_path, 'w') as f:\n",
        "    f.write(content)\n",
        "with open(In_between_output, 'w') as f:\n",
        "    f.write(content)\n",
        "with open(adib_summary, 'w') as f:\n",
        "    f.write(content)\n",
        "with open(adib_summary2, 'w') as f:\n",
        "    f.write(content)\n",
        "with open(adib_titles, 'w') as f:\n",
        "    f.write(content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw3Xtod4m2Ew"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aofzVY33RdUI"
      },
      "outputs": [],
      "source": [
        "!pip install PyMuPDF\n",
        "\n",
        "import re\n",
        "import fitz\n",
        "\n",
        "\n",
        "categories = {\n",
        "    'Environment': ['sustainability', 'environment', 'energy usage and efficiency', 'climate change strategy', 'waste reduction', 'biodiversity loss', 'greenhouse gas emissions', 'carbon food print reduction'],\n",
        "    'Social': ['social', 'fair pay and living wages', 'equal employment opportunity', 'employee benefits', 'workplace health and safety', 'community engagement', 'responsible supply chain partnerships', 'adhering to labor'],\n",
        "    'Governance': ['governance', 'corporate governance', 'risk management', 'compliance', 'ethical business practices', 'avoiding conflicts of interest', 'accounting integrity and transparency']\n",
        "    }\n",
        "\n",
        "def pdf_to_text_All(pdf_path, all_text_path):\n",
        "    # Open the PDF file in read-binary mode\n",
        "    doc = fitz.open(pdf_path)\n",
        "    unicode_chars_pattern = r'[\\u201c\\u201d\\u2019s]'\n",
        "    social_count = 0\n",
        "    text =\"\"\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "\n",
        "            for block in blocks:\n",
        "                if block['type'] == 0:  # text block\n",
        "                    for line in block[\"lines\"]:\n",
        "                        for span in line[\"spans\"]:\n",
        "                            text += span[\"text\"]\n",
        "                            text += \"\\n\"\n",
        "\n",
        "        text = re.sub(unicode_chars_pattern, '', text)\n",
        "        text = re.sub(r'\\n', ' ', text)  # Remove newlines\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    with open(all_text_path, 'w', encoding='utf-8') as txt_file:\n",
        "        txt_file.write(text)\n",
        "    return text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1JDzN45oh40"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2\n",
        "import PyPDF2\n",
        "def extract_first_pages(pdf_path, title_txt, num_pages=5):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    unicode_chars_pattern = r'[\\u201c\\u201d\\u2019s]'\n",
        "    social_count = 0\n",
        "    text =\"\"\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        for page_num in range(min(num_pages, len(doc))):\n",
        "            page = doc.load_page(page_num)\n",
        "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "\n",
        "            for block in blocks:\n",
        "                if block['type'] == 0:  # text block\n",
        "                    for line in block[\"lines\"]:\n",
        "                        for span in line[\"spans\"]:\n",
        "                            text += span[\"text\"]\n",
        "                            text += \"\\n\"\n",
        "    with open(title_txt, 'w', encoding='utf-8') as txt_file:\n",
        "        txt_file.write(text)\n",
        "    print(text)\n",
        "    return text\n",
        "\n",
        "#extract_first_pages('/content/drive/MyDrive/DocumentSummarizer/adib-esg-report-2023.pdf', '/content/drive/MyDrive/DocumentSummarizer/adib_titles.txt', 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU2rw4riwv00"
      },
      "outputs": [],
      "source": [
        "  def split_content_by_headlines2(title_txt):\n",
        "      doc = fitz.open(title_txt)\n",
        "      pattern = re.compile(r'\\.{2,}\\s*\\d+[A-Z\\s]*')\n",
        "      content_groups = []\n",
        "      current_group = {}\n",
        "      current_group[\"title\"] = \"\"\n",
        "      current_group[\"content\"] = \"\"\n",
        "\n",
        "      for page_num in range(len(doc)):\n",
        "          page = doc.load_page(page_num)\n",
        "          blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "\n",
        "          for block in blocks:\n",
        "              if block['type'] == 0:  # text block\n",
        "                  for line in block[\"lines\"]:\n",
        "                      for span in line[\"spans\"]:\n",
        "                          text = span[\"text\"].strip()\n",
        "                          #print(text)\n",
        "                          # Check if the text is in all capital letters\n",
        "                          if text.isupper() and len(text) > 2 and not pattern.match(text):  # Adjust minimum length as needed\n",
        "                              if current_group[\"content\"]:\n",
        "                                  content_groups.append(current_group)\n",
        "                                  current_group = {\"title\": \"\", \"content\": \"\"}\n",
        "                              current_group[\"title\"] = text\n",
        "                          elif pattern.match(text):\n",
        "                              current_group[\"content\"] += text + \" \"\n",
        "                          else:\n",
        "                              current_group[\"content\"] += text + \" \"\n",
        "\n",
        "\n",
        "      # Remove the dublication if found\n",
        "      seen_titles = set()\n",
        "      unique_data = []\n",
        "      for current_group[\"title\"], current_group[\"content\"] in content_groups:\n",
        "          if current_group[\"title\"] not in seen_titles:\n",
        "              current_group[\"title\"] = \"last_title\"\n",
        "              current_group[\"content\"] = \"last_content\"\n",
        "              unique_data.append(content_groups)\n",
        "              seen_titles.add(current_group[\"title\"])\n",
        "      print(unique_data)\n",
        "      return content_groups\n",
        "\n",
        "  #reult = split_content_by_headlines2('/content/drive/MyDrive/DocumentSummarizer/adib_titles.txt')  #'/content/drive/MyDrive/DocumentSummarizer/'\n",
        "  #print(rseult)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1K_-XXzwUmb"
      },
      "outputs": [],
      "source": [
        "def append_content_to_titles(text, content_groups, output_txt2, file_summary, file_summary2):\n",
        "    titles = [group[\"title\"] for group in content_groups]  # Create an array with just the titles\n",
        "    title_count = len(titles)  # Count the number of titles\n",
        "\n",
        "    catogry = \"\"\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    concatenated_text = ' '.join(lines)\n",
        "\n",
        "\n",
        "    with open(output_txt2, 'w', encoding='utf-8') as txt_file,\\\n",
        "         open(file_summary, 'w', encoding='utf-8') as txt_file2,\\\n",
        "         open(file_summary2, 'w', encoding='utf-8') as txt_file3:\n",
        "        for i in range(0, len(titles) - 1):\n",
        "                first_headline = titles[i]\n",
        "                if first_headline == \"last_title\": continue\n",
        "                headline_length = len(first_headline)\n",
        "                start_indices = [m.start() for m in re.finditer(re.escape(first_headline), concatenated_text)]\n",
        "                start_after_headline = start_indices[0] + headline_length\n",
        "                concatenated_text = concatenated_text[start_after_headline:]\n",
        "\n",
        "        #txt_file.write(concatenated_text + \"\\n\")\n",
        "        summaries = []\n",
        "        Fsummaries = []\n",
        "        for i in range(0, len(titles) -1):\n",
        "            first_headline = titles[i]\n",
        "            try:\n",
        "                second_headline = titles[i+1]\n",
        "            except IndexError:\n",
        "                second_headline = first_headline\n",
        "            except Exception as e:\n",
        "                second_headline = first_headline\n",
        "\n",
        "\n",
        "            print(\"**************\\n\")\n",
        "            print(first_headline)\n",
        "            print(\"\\n\",second_headline)\n",
        "            #txt_file.write(\"\\n \" + first_headline + \".\"*50 + \"\\n\")\n",
        "\n",
        "            c = 0\n",
        "            x = 0\n",
        "            start_indices = [m.start() for m in re.finditer(re.escape(first_headline), concatenated_text)]\n",
        "            print(start_indices)\n",
        "            print(len(start_indices))\n",
        "            last = len(titles) - 2\n",
        "            if (i == last and second_headline == \"last_title\") or (i == len(titles) and first_headline == \"last_title\"):\n",
        "                in_between = concatenated_text[start_indices[c]:]\n",
        "                if not in_between.strip(): continue\n",
        "                catogry = categorize_text(in_between, categories)\n",
        "                if catogry != \"Not_Found\" or catogry != \"\":\n",
        "                  #summary = summarize_text(in_between, True)\n",
        "                  #Fsummary = summarize_text(summary, False)\n",
        "                  #summaries.append(summary)\n",
        "                  #Fsummaries.append(Fsummary)\n",
        "                  #print(\"Fsummaries\", Fsummaries)\n",
        "                  #txt_file2.write(summary + \"\\n\")\n",
        "                  #txt_file3.write(Fsummary + \"\\n\")\n",
        "                  print(\"catogry is:\", catogry)\n",
        "                print(\"content of: \", first_headline)\n",
        "                txt_file.write(in_between + \"\\n\")\n",
        "                #txt_file.write(\"\\n NEW_PRAG\" + \".\"*50 + \"\\n\")\n",
        "                break\n",
        "\n",
        "            if len(start_indices) == 0:\n",
        "                print(\"Zero section for start\")\n",
        "                start = find_partial_matches(concatenated_text, first_headline)\n",
        "                if start != 0:\n",
        "                    start_indices = [start]\n",
        "                    print(\"match start\", start)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            if first_headline == second_headline:\n",
        "                continue\n",
        "            else:\n",
        "                end_indices = [m.start() for m in re.finditer(re.escape(second_headline), concatenated_text)]\n",
        "                print(end_indices)\n",
        "                print(len(end_indices))\n",
        "                if len(end_indices) == 0:\n",
        "                    print(\"Zero section for end\")\n",
        "                    end = find_partial_matches(concatenated_text, second_headline)\n",
        "                    print(end)\n",
        "                    if end != 0:\n",
        "                        end_indices = [end]\n",
        "                        end_indices = check_indicator(start_indices, end_indices)\n",
        "                        in_between = concatenated_text[start_indices[c]:end_indices[x]]\n",
        "                        if not in_between.strip(): continue\n",
        "                        catogry = categorize_text(in_between, categories)\n",
        "                        print(catogry)\n",
        "                        if catogry != \"Not_Found\" or catogry != \"\":\n",
        "                            #summary = summarize_text(in_between, True)\n",
        "                            #Fsummary = summarize_text(summary, False)\n",
        "                            #summaries.append(summary)\n",
        "                            #Fsummaries.append(Fsummary)\n",
        "                            #print(\"Fsummaries\", Fsummaries)\n",
        "                            #txt_file2.write(summary + \"\\n\")\n",
        "                            #txt_file3.write(Fsummary + \"\\n\")\n",
        "                            print(\"catogry is:\", catogry)\n",
        "                        print(\"content of: \", first_headline)\n",
        "                        txt_file.write(in_between + \"\\n\")\n",
        "\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                else:\n",
        "                    end_indices = check_indicator(start_indices, end_indices)\n",
        "                    in_between = concatenated_text[start_indices[c]:end_indices[x]]\n",
        "                    if not in_between.strip(): continue\n",
        "                    catogry = categorize_text(in_between, categories)\n",
        "                    if catogry != \"Not_Found\" or catogry != \"\":\n",
        "                        #summary = summarize_text(in_between, True)\n",
        "                        #Fsummary = summarize_text(summary, False)\n",
        "                        #summaries.append(summary)\n",
        "                        #Fsummaries.append(Fsummary)\n",
        "                        #print(\"Fsummaries\", Fsummaries)\n",
        "                        #txt_file2.write(summary + \"\\n\")\n",
        "                        #txt_file3.write(Fsummary + \"\\n\")\n",
        "                        print(\"catogry is:\", catogry)\n",
        "                    print(\"content of: \", first_headline)\n",
        "                    txt_file.write(in_between + \"\\n\")\n",
        "\n",
        "\n",
        "    #print(Fsummaries)\n",
        "    return summaries\n",
        "\n",
        "def check_indicator(start_indices, end_indices):\n",
        "    endicator = end_indices\n",
        "    if end_indices[0] < start_indices[0]:\n",
        "        if len(end_indices) > 1:\n",
        "            for l in range(0, len(end_indices)):\n",
        "                if start_indices[0] < end_indices[l]:\n",
        "                    endicator = [end_indices[l]]\n",
        "    return endicator\n",
        "\n",
        "def find_partial_matches(text, headline):\n",
        "    # Split the title into words\n",
        "    title_words = headline.split()\n",
        "    # Convert the text to lower case for case-insensitive comparison\n",
        "    text_lower = text\n",
        "    # Check if any word from the title is present in the text\n",
        "    start_position = 0\n",
        "    for word in title_words:\n",
        "        match = re.search(r'\\b' + re.escape(word) + r'\\b', text_lower, re.IGNORECASE)\n",
        "        if match:\n",
        "            start_position = match.start()\n",
        "            break\n",
        "        else:\n",
        "            continue\n",
        "    return start_position\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase and remove special characters\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def categorize_text(text, categories):\n",
        "    # Preprocess the text\n",
        "    text = preprocess_text(text)\n",
        "    # Dictionary to keep track of keyword matches for each category\n",
        "    category_matches = {category: 0 for category in categories}\n",
        "    # Check for keyword matches in each category\n",
        "    for category, keywords in categories.items():\n",
        "        for keyword in keywords:\n",
        "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
        "                category_matches[category] += 1\n",
        "\n",
        "    # Find the category with the highest match count\n",
        "    max_category = max(category_matches, key=category_matches.get)\n",
        "    max_count = category_matches[max_category]\n",
        "\n",
        "    # Return the category with the most matches or None if no matches\n",
        "    if max_count > 0:\n",
        "        return max_category\n",
        "    else:\n",
        "        return \"Not_Found\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC0LyAPx21A5"
      },
      "outputs": [],
      "source": [
        "# Get the content of document and write the output in .txt file\n",
        "\n",
        "headlines_file_extract = extract_first_pages(target_path, adib_titles, 5)\n",
        "headlines = split_content_by_headlines2(adib_titles)\n",
        "\n",
        "All_document = pdf_to_text_All(target_path, all_text_path)\n",
        "summaries = append_content_to_titles(All_document, headlines, In_between_output, adib_summary, adib_summary2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxC07n1yx6iB"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
        "!pip install transformers\n",
        "!pip install numpy==1.24.3\n",
        "\n",
        "!pip install numpy scipy scikit-learn\n",
        "\n",
        "from transformers import pipeline\n",
        "import textwrap\n",
        "\n",
        "def summarize_text(text, bolean):\n",
        "  summaries = []\n",
        "  model='facebook/bart-large-cnn'\n",
        "  max_length = 1024\n",
        "  overlap = 100\n",
        "  try:\n",
        "      chunks = textwrap.wrap(text, width=max_length - overlap)\n",
        "      #chunks =  split_line_into_chunks(text)\n",
        "      #print(chunks)\n",
        "\n",
        "      summaries = []\n",
        "      for chunk in chunks:\n",
        "          text_length = len(chunk)\n",
        "          if text_length < 100:\n",
        "            if text_length < 10: continue\n",
        "            summary_max_length = 10\n",
        "            summary_min_length = 5\n",
        "          elif text_length < 1025:\n",
        "            summary_max_length = 50\n",
        "            summary_min_length = 10\n",
        "          else:\n",
        "            summary_max_length = 20\n",
        "            summary_min_length = 5\n",
        "          if bolean and text_length > 100:\n",
        "            summary_max_length = max(1, int(text_length * 0.05))  # Ensure it's at least 1\n",
        "            summary_min_length = min(1, int(text_length * 0.01))\n",
        "\n",
        "          #print(\"summary_max_length\", summary_max_length)\n",
        "          summarizer2 = pipeline('summarization', model=model)\n",
        "          summary = summarizer2(chunk, max_length=summary_max_length, min_length=summary_min_length, do_sample=False)\n",
        "          summaries.append(summary)\n",
        "          #print(\"sumarry: \", summary)\n",
        "\n",
        "      #print(\"end summary loop\")\n",
        "      #print(summaries)\n",
        "      combined_summary = \" \".join([summary[0]['summary_text'] for summary in summaries])\n",
        "      #print(\"combined_summary\", combined_summary)\n",
        "      return combined_summary\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"Error during summarization: {e}\")\n",
        "      return \"no sumarize text\"\n",
        "\n",
        "#sum_text = \"IB financing is increasingly geared towards a sustainable economy. 13 green financing transactions were closed, including AED1.7 billion green financing for three sustainable sewage treatment plants across Saudi Arabia. ADIB strives to create wider positive impacts for the society. This is achieved through its ethical Shari’a principles and responsible, innovative products and services that make banking more affordable and sustainable. Dubai has demonstrated one of the lowest levelized cost of electricity in the world of USD cents 1.6953 per kWh. Project involves the construction of a state-of-the-art 900MW solar PV plant, using bi-facial panels. Shuaa Energy 3 P.S.C. was structured as a c. 27-year soft mini perm financing with both conventional and Islamic tranches. The financing structure featured a set of equity bridge finances provided by local banks and also by DEWA. Jubail-3A IWP will utilize reverse osmosis (RO) technology to yield a capacity of 600,000 cubic meters a day. The project will benefit from a 25-year Water Purchase Agreement (WPA) with Saudi Water Partnership Company (SWPA) The plant is located in Jubail, Saudi Arabia Jazlah Water Desalination Company USD 481 million September 2020 Construction of Water Treatment Plant 26 Years Year 6 Istina ljara Hogan Lovells Covington Mandated Lead Arranger. ADIB has invested AED 1.8 billion in sustainable/green Sukuks. 93.6% of financing provided to corporates assets were subject to a combined social or environmental screening. A total value of USD 1.7 billion in sustainable project financing has been identified. ADIB’s Credit Risk Policy will be updated to include an ESG insert.  ADIB is dedicated to growing its reputation as a leading Islamic bank, exercising sound governance practices that focus on Shari’a compliance. Measuring portfolio carbon emissions is essential to be able to reduce them. Shari’a financing has steadily increased, amounting to AED 113 billion in 2022, a 22% increase on 2021. All employees are trained in these criteria, with a total of 104 hours delivered in 2022 (an increase of 4% on 2021) ADIB portfolio held around AED 1.8 billion of Green Sukuk in 2022. Proceeds from both were invested in projects such as renewable energy, energy efficiency, sustainable water management and green buildings. In 2022 the bank accelerated efforts against its three-year Digital Transformation Strategy. Examples of innovation initiatives in 2022 included expanding Interactive Teller Machines. Facial recognition artificial intelligence (AI) was used in 40% of account openings in 2022. The proportion of digitally active customers increased to 76%, with one new digital product and a 31.1% rise in online or mobile transactions. ADIB launched ACE, the region’s first digital command centre. Digital Excellence remains at the heart of ADIB’s 2025 strategy. The bank strives to become a digital-first financial institution. For customers, this means an even more seamless experience.\"\n",
        "#Sum2_txt=  \"ADIB strives to create wider positive impacts for the society. 13 green financing transactions were closed, including AED1.7 billion green financing for three sustainable sewage treatment plants across Saudi Arabia. ADIB has invested AED 1.8 billion in sustainable/green Sukuks. 93.6% of financing provided to corporates assets were subject to a combined social or environmental screening. A total value of USD 1.7 billion inustainable project financing has been identified. ADIB portfolio held around AED 1.8 billion of Green Sukuk in 2022. Shari’a financing has steadily increased, amounting to AED 113 billion in 2022, a 22% increase on 2021. ADIB aims to become a digital-first financial institution. For customers, this means an even more seamless experience.\"\n",
        "#summarize_text(sum_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5KkQQ0PVWnS"
      },
      "outputs": [],
      "source": [
        "def read_as_spilit(file):\n",
        "  with open(file, \"r\") as fp:\n",
        "    str = fp.read().splitlines()\n",
        "\n",
        "  return str\n",
        "\n",
        "def read_as_text(file):\n",
        "  with open(file, \"r\") as fp:\n",
        "    str = fp.read()\n",
        "  return str\n",
        "\n",
        "#read_as_spilit('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-3n-Vs7SONl"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "\n",
        "def sum_google_peg(documents_):\n",
        "  topics = get_top_topic_and_normalize(documents_)\n",
        "  print(\"topics:\", topics)\n",
        "  model_name = \"google/pegasus-xsum\"\n",
        "  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "  model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "  REPORT = \"ESG Report\"\n",
        "  ORGANIZATION = pdf_path\n",
        "  tokens = tokenizer(\" \".join(topics) + f\" Report / Study Named {REPORT} By {ORGANIZATION}\", return_tensors=\"pt\", truncation=True)\n",
        "  print(\"tokens:\", tokens)\n",
        "  summary_ids = model.generate(tokens[\"input_ids\"], max_length=1500, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "  summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "  return summary\n",
        "\n",
        "#documents = read_as_spilit('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "#sum_google_peg(documents)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVN5EflVDxWG"
      },
      "outputs": [],
      "source": [
        "def re_summarizing(all_text_path, In_between_text, file_summary):\n",
        "  processed_text = process_text(In_between_text)\n",
        "  summaries = []\n",
        "  with open(file_summary, 'w', encoding='utf-8') as txt_file:\n",
        "    for line_chunks in processed_text:\n",
        "      summary = sum_google_peg(line_chunks)\n",
        "      summaries.append(summary)\n",
        "      print(\"summaries\", summaries)\n",
        "      txt_file.write(summary + \"\\n\")\n",
        "  return summaries\n",
        "\n",
        "\n",
        "def split_text_by_newlines(text):\n",
        "  return text.splitlines()\n",
        "\n",
        "\n",
        "def split_line_into_chunks(line):\n",
        "  sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', line)\n",
        "  chunks = [sentence.strip() for sentence in sentences if sentence]\n",
        "  #chunks = [line[i:i+width] for i in range(0, len(line), width)]\n",
        "  return chunks\n",
        "\n",
        "def process_text(text):\n",
        "  lines = split_text_by_newlines(text)\n",
        "  result = [split_line_into_chunks(line) for line in lines]\n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFRjbwnyBP2f"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud\n",
        "!pip install matplotlib\n",
        "\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def word_cloud(sum_text):\n",
        "    # Generate a word cloud\n",
        "    file_name = \"word_cloud.png\"\n",
        "    file_path = f'{target_dir}{file_name}.png'\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='black', max_words=30, stopwords=STOPWORDS).generate(sum_text)\n",
        "    # Display the word cloud\n",
        "    plt.figure(figsize=(5, 3))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    #plt.show()\n",
        "\n",
        "    # Save to a BytesIO object\n",
        "    wordcloud.to_image().save(file_path, format='png')\n",
        "\n",
        "\n",
        "    plt.savefig(file_path, format='png')\n",
        "    plt.close()\n",
        "\n",
        "    return file_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nQE0eI4CThd"
      },
      "outputs": [],
      "source": [
        "# Install nltk (if needed)\n",
        "!pip install nltk\n",
        "import nltk\n",
        "# Download the stopwords corpus\n",
        "nltk.download('stopwords')\n",
        "# Import stopwords and test\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    return text.lower().strip()\n",
        "\n",
        "def normalize_terms(terms):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  normalized = []\n",
        "  for term in terms:\n",
        "      # Convert to lowercase\n",
        "      term = term.lower()\n",
        "      # Remove punctuation\n",
        "      term = term.translate(str.maketrans('', '', string.punctuation))\n",
        "      # Remove stop words\n",
        "      if term not in stop_words:\n",
        "          # Stemming\n",
        "          term = stemmer.stem(term)\n",
        "          normalized.append(term)\n",
        "  return normalized\n",
        "\n",
        "\n",
        "def get_top_words_for_topics(model, feature_names, n_words=10):\n",
        "    topics = model.components_\n",
        "    top_words = {}\n",
        "    for i, topic in enumerate(topics):\n",
        "        top_indices = topic.argsort()[-n_words:][::-1]\n",
        "        top_words[f\"Topic {i+1}\"] = [feature_names[index] for index in top_indices]\n",
        "    return top_words\n",
        "\n",
        "\n",
        "\n",
        "def get_top_word_and_normalize(documents):\n",
        "  vectorizer = CountVectorizer()\n",
        "  X = vectorizer.fit_transform(documents)\n",
        "\n",
        "  lda = LatentDirichletAllocation(n_components=3, random_state=0)\n",
        "  lda.fit(X)\n",
        "\n",
        "  feature_names = vectorizer.get_feature_names_out()\n",
        "  top_words_per_topic = get_top_words_for_topics(lda, feature_names)\n",
        "  top_words_per_topic = {topic: normalize_terms(words) for topic, words in top_words_per_topic.items()}\n",
        "\n",
        "  return top_words_per_topic\n",
        "\n",
        "def get_top_topic_and_normalize(documents):\n",
        "  top_words_per_topic = get_top_word_and_normalize(documents)\n",
        "  topics = set()\n",
        "  for x in top_words_per_topic:\n",
        "    for y in top_words_per_topic[x]:\n",
        "      topics.add(y)\n",
        "  top_topic = topics\n",
        "  top_topic_wo  = top_words_per_topic\n",
        "  return topics\n",
        "#documents = read_as_spilit('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "#print(get_top_word_and_normalize(documents))\n",
        "#print(get_top_topic_and_normalize(documents))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_2wFgFT5ScT"
      },
      "outputs": [],
      "source": [
        "# sumarize the extract text and write the out put in txt file\n",
        "text = read_as_text(In_between_output)\n",
        "summary = re_summarizing(all_text_path, text, adib_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTGfq16aCrpT"
      },
      "outputs": [],
      "source": [
        "!pip install mglearn\n",
        "import io\n",
        "import sys\n",
        "import mglearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def classification(DATA):\n",
        "  # Define the Vectorizer\n",
        "  VECT: CountVectorizer = CountVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
        "  # Transform the data to create the document-term matrix\n",
        "  FIN = VECT.fit_transform(DATA)\n",
        "  feature_names = VECT.get_feature_names_out()\n",
        "  # Convert to DataFrame for inspection\n",
        "  pd.DataFrame(FIN.toarray(), columns=VECT.get_feature_names_out()).head(1)\n",
        "  # Fit the LDA model\n",
        "  LDA: LatentDirichletAllocation = LatentDirichletAllocation(n_components=3, random_state=42)\n",
        "  LDA_DTF = LDA.fit_transform(FIN)\n",
        "\n",
        "  topics = []\n",
        "  for idx, topic in enumerate(LDA.components_):\n",
        "      top_features_ind = topic.argsort()[:-10 - 1:-1]\n",
        "      top_features = [feature_names[i] for i in top_features_ind]\n",
        "      topics.append(top_features)\n",
        "\n",
        "  print(topics)\n",
        "  predefined_categories = categories\n",
        "  topic_category_mapping = {}\n",
        "  category_scores = {}\n",
        "  for category, keywords in predefined_categories.items():\n",
        "    # Calculate overlap between topic words and category keywords\n",
        "    score = len(set(topic) & set(keywords))\n",
        "    category_scores[category] = score\n",
        "\n",
        "  # Choose the category with the highest overlap score\n",
        "  mapped_category = max(category_scores, key=category_scores.get)\n",
        "\n",
        "  # Store the topic and its mapped category in the dictionary\n",
        "  topic_category_mapping[f\"Topic {idx+1}\"] = {\n",
        "      \"top_words\": topic,\n",
        "      \"category\": mapped_category\n",
        "  }\n",
        "\n",
        "  for topic_key in topic_category_mapping:\n",
        "    category_value = topic_category_mapping[topic_key]['category']\n",
        "\n",
        "  return category_value\n",
        "\n",
        "def classification2(DATA):\n",
        "    # Define the Vectorizer\n",
        "  VECT: CountVectorizer = CountVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
        "  # Transform the data to create the document-term matrix\n",
        "  FIN = VECT.fit_transform(DATA)\n",
        "  feature_names = VECT.get_feature_names_out()\n",
        "  # Convert to DataFrame for inspection\n",
        "  pd.DataFrame(FIN.toarray(), columns=VECT.get_feature_names_out()).head(1)\n",
        "  # Fit the LDA model\n",
        "\n",
        "  LDA: LatentDirichletAllocation = LatentDirichletAllocation(n_components=3)\n",
        "  LDA_DTF = LDA.fit_transform(FIN)\n",
        "\n",
        "  sorting = np.argsort(LDA.components_)[:, ::-1]\n",
        "  features = np.array(VECT.get_feature_names_out())\n",
        "  array = np.full((1, sorting.shape[1]), 1)\n",
        "  array = np.concatenate((array, sorting), axis=0)\n",
        "\n",
        "  output = io.StringIO()\n",
        "  # Redirect stdout to capture print output\n",
        "  original_stdout = sys.stdout\n",
        "  sys.stdout = output\n",
        "  try:\n",
        "    # Call print_topics and capture the output\n",
        "    mglearn.tools.print_topics(\n",
        "        topics=range(1, 4),\n",
        "        feature_names=features,\n",
        "        sorting=array,\n",
        "        topics_per_chunk=5,\n",
        "        n_words=10\n",
        "    )\n",
        "  finally:\n",
        "      # Restore the original stdout\n",
        "      sys.stdout = original_stdout\n",
        "\n",
        "  # Retrieve the captured text\n",
        "  captured_text = output.getvalue()\n",
        "  print(captured_text)\n",
        "  output.close()\n",
        "\n",
        "  return captured_text\n",
        "\n",
        "#DATA = read_as_spilit('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "#topicss = classification(DATA)\n",
        "\n",
        "\n",
        "#print(topicss)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccQd_R3kEaDt"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download 'punkt' model\n",
        "nltk.download('punkt')\n",
        "\n",
        "#documents = read_as_spilit('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "#documents2 = read_as_text('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "\n",
        "def Bar_Chart_of_Keyword_Frequency(sum_text, keywords_main, file_name):\n",
        "    file_path = f'{target_dir}{file_name}.png'\n",
        "    if keywords_main == 'Environment' :\n",
        "        keywords = ['environment', 'sustainability', 'energy usage and efficiency', 'climate change strategy', 'waste reduction', 'biodiversity loss', 'greenhouse gas emissions', 'carbon food print reduction']\n",
        "    elif keywords_main == 'Social' :\n",
        "        keywords = ['social', 'fair pay and living wages', 'equal employment opportunity', 'employee benefits', 'workplace health and safety', 'community engagement', 'responsible supply chain partnerships', 'adhering to labor']\n",
        "    elif keywords_main == 'Governance' :\n",
        "        keywords = ['governance', 'corporate governance', 'risk management', 'compliance', 'ethical business practices', 'avoiding conflicts of interest', 'accounting integrity and transparency']\n",
        "    else:\n",
        "        keywords = []\n",
        "    # Tokenize the summary text and remove stopwords\n",
        "    nltk.download('stopwords')\n",
        "    tokens = nltk.word_tokenize(sum_text.lower())\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation and numbers\n",
        "    filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "\n",
        "\n",
        "    # Count the frequency of each word\n",
        "    word_freq = Counter(filtered_tokens)\n",
        "    # Plot the frequency of the top 10 words\n",
        "    #most_common_words = word_freq.most_common(10)\n",
        "    #words, counts = zip(*most_common_words)\n",
        "\n",
        "    filtered_word_freq = {word: word_freq[word] for word in keywords}\n",
        "\n",
        "    words = list(filtered_word_freq.keys())\n",
        "    frequencies = list(filtered_word_freq.values())\n",
        "\n",
        "    #if you need most common word without frequency\n",
        "    #most_common_words_only = [word for word, freq in most_common_words]\n",
        "    #print(most_common_words_only)\n",
        "\n",
        "    plt.figure(figsize=(4, 2))\n",
        "    plt.barh(words, frequencies, color='skyblue')\n",
        "\n",
        "    #plt.bar(words, counts, color='skyblue')\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel('Words')\n",
        "    plt.title(keywords_main)\n",
        "    plt.subplots_adjust(left=0.3, right=0.7)\n",
        "    plt.tight_layout()\n",
        "    #plt.show()\n",
        "    # Save to a BytesIO object\n",
        "    plt.savefig(file_path, format='png')\n",
        "    plt.close()\n",
        "\n",
        "    return file_path\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase and remove special characters\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def categorize_text(text, categories):\n",
        "    # Preprocess the text\n",
        "    text = preprocess_text(text)\n",
        "\n",
        "    # Dictionary to keep track of keyword matches for each category\n",
        "    category_matches = {category: 0 for category in categories}\n",
        "    # Check for keyword matches in each category\n",
        "    for category, keywords in categories.items():\n",
        "        for keyword in keywords:\n",
        "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
        "                category_matches[category] += 1\n",
        "\n",
        "    # Find the category with the highest match count\n",
        "    max_category = max(category_matches, key=category_matches.get)\n",
        "    max_count = category_matches[max_category]\n",
        "\n",
        "    #print(\"max_category\", max_category)\n",
        "    #print(\"max_count\", max_count)\n",
        "    # Return the category with the most matches or None if no matches\n",
        "    if max_count > 0:\n",
        "        return max_category\n",
        "    else:\n",
        "        return \"Not_Found\"\n",
        "def read_as_text(file):\n",
        "  with open(file, \"r\") as fp:\n",
        "    str = fp.read()\n",
        "  return str\n",
        "\n",
        "#keywords = categorize_text(documents2, categories)\n",
        "#Bar_Chart_of_Keyword_Frequency(documents2,keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltVeAIllFnx-"
      },
      "outputs": [],
      "source": [
        "def highlight_text(sum_text, keywords_main):\n",
        "  lines = split_text_by_newlines(sum_text)\n",
        "  final_sum_text = \"\"\n",
        "  for line in lines:\n",
        "    sum_text = line.lower()\n",
        "    keywords = ['energy usage and efficiency', 'climate change strategy', 'waste reduction', 'biodiversity loss', 'greenhouse gas emissions', 'carbon food print reduction', 'fair pay and living wages', 'equal employment opportunity', 'employee benefits', 'workplace health and safety', 'community engagement', 'responsible supply chain partnerships', 'adhering to labor', 'governance', ' social', 'environmental', 'corporate governance', 'risk management', 'compliance', 'ethical business practices', 'avoiding conflicts of interest', 'accounting integrity and transparency']\n",
        "\n",
        "    for keyword in keywords:\n",
        "        sum_text = re.sub(rf'({keyword})', r'<b><font color=\"red\">\\1</font></b>', sum_text, flags=re.IGNORECASE)\n",
        "    final_sum_text += sum_text +\"<br/>\"\n",
        "  print(final_sum_text)\n",
        "  return final_sum_text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xMVQ0T3_Zji"
      },
      "outputs": [],
      "source": [
        "# For PDF generation\n",
        "!pip install reportlab matplotlib wordcloud pillow numpy\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Image\n",
        "from reportlab.platypus import Spacer\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def create_pdf(documents, documents2, categories):\n",
        "    pdf_file_name = \"report.pdf\"\n",
        "    pdf_path = f'{target_dir}{pdf_file_name}'\n",
        "    #bar_chart_file = 'bar_chart.png'\n",
        "    doc = SimpleDocTemplate(pdf_path, pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    normal_style = styles[\"Normal\"]\n",
        "    title_style = styles[\"Heading2\"]  # Use a different style for section titles\n",
        "    subtitle_style = styles[\"Heading3\"]  # Use a smaller heading style for subtitles\n",
        "    elements = []\n",
        "    keywords = categorize_text(documents, categories)\n",
        "    # Title and Date\n",
        "    title = \"ESG Summary Report\"\n",
        "    date_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # Add title\n",
        "    elements.append(Paragraph(title, styles[\"Title\"]))\n",
        "    elements.append(Paragraph(f\"Date: {date_str}\", normal_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the title\n",
        "\n",
        "    # Add the title for the highlighted text section\n",
        "    elements.append(Paragraph(\"Abstract_text\", title_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the subtitle\n",
        "\n",
        "    # Combine highlighted text into one string\n",
        "    combined_highlighted_text = highlight_text(documents, keywords)\n",
        "\n",
        "    # Format the combined text to include bold and red styling\n",
        "    highlighted_paragraph = f'{combined_highlighted_text}'\n",
        "    elements.append(Paragraph(highlighted_paragraph, normal_style))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    elements.append(Paragraph(\"General Categorization\", title_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the subtitle\n",
        "    category_paragraph = f'1- The classification depend on most matches keyword and sub keyword, so from that point of view the most match category is: <font color=\"red\">{keywords}</font>'\n",
        "    elements.append(Paragraph(category_paragraph, normal_style))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    name = f'{keywords}_bar_chart'\n",
        "    bar_chart_file1 = Bar_Chart_of_Keyword_Frequency(documents, keywords, name)\n",
        "    print(\"output:\",bar_chart_file1)\n",
        "    # Add an Image\n",
        "    if os.path.exists(bar_chart_file1):\n",
        "        img = Image(bar_chart_file1, width=500, height=300)\n",
        "        elements.append(img)\n",
        "        elements.append(Spacer(1, 12))\n",
        "    else:\n",
        "        error_text = Paragraph(\"Error: Image not found.\", normal_style)\n",
        "        elements.append(error_text)\n",
        "        elements.append(Spacer(1, 12))\n",
        "\n",
        "    topicss = classification(documents2)\n",
        "    topicss_paragraph = f'2- The classification depend on Calculate overlap between topic words and category keywords, so from that point of view the most match category is: <font color=\"red\">{topicss}</font>'\n",
        "    elements.append(Paragraph(topicss_paragraph, normal_style))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    top_word = get_top_word_and_normalize(documents2)\n",
        "    top_topic = get_top_topic_and_normalize(documents2)\n",
        "    elements.append(Paragraph(\"Top Topic\", title_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the subtitle\n",
        "    top_topic_paragraph = f'The most top topic are: {top_topic}'\n",
        "    elements.append(Paragraph(top_topic_paragraph, normal_style))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    elements.append(Paragraph(\"Top Word Per Topic\", title_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the subtitle\n",
        "    top_word_paragraph = f'{top_word}'\n",
        "    elements.append(Paragraph(top_word_paragraph, normal_style))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    elements.append(Paragraph(\"Classification Topic on General\", title_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the subtitle\n",
        "    topicssg = classification2(documents2)\n",
        "    for line in topicssg.split('\\n'):\n",
        "      if line.strip():  # Only add non-empty lines\n",
        "          paragraph = Paragraph(line, normal_style)\n",
        "          elements.append(paragraph)\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    elements.append(Paragraph(\"Visulaization\", title_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the subtitle\n",
        "\n",
        "\n",
        "    word_cloud_file = word_cloud(documents)\n",
        "    if os.path.exists(word_cloud_file):\n",
        "        img = Image(word_cloud_file, width=500, height=300)\n",
        "        elements.append(img)\n",
        "        elements.append(Spacer(1, 12))\n",
        "    else:\n",
        "        error_text = Paragraph(\"Error: Image not found.\", normal_style)\n",
        "        elements.append(error_text)\n",
        "        elements.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "\n",
        "    # Build the PDF\n",
        "    doc.build(elements)\n",
        "\n",
        "# Generate the final summary\n",
        "#sum_google_peg(documents)\n",
        "\n",
        "documents = read_as_text(adib_summary)\n",
        "#sub_documents = read_as_text(adib_summary2)\n",
        "documents2 = read_as_spilit(adib_summary)\n",
        "#sub_documents2 = read_as_spilit(adib_summary2)\n",
        "#sum_text = summarize_text(documents)\n",
        "sum_text = \"\"\n",
        "\n",
        "create_pdf(documents, documents2, categories)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}